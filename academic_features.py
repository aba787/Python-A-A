
# ðŸŽ“ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
# Academic Features for Final Project

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import sqlite3

class AcademicAnalytics:
    """
    Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ø§Ù„Ù…ØªØ·ÙˆØ±
    Advanced Academic Analytics System
    """
    
    def __init__(self, chatbot_instance):
        self.chatbot = chatbot_instance
        self.setup_academic_database()
    
    def setup_academic_database(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©"""
        conn = sqlite3.connect(self.chatbot.db_path)
        cursor = conn.cursor()
        
        # Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS academic_metrics (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            model_type TEXT NOT NULL,
            language_detected TEXT,
            intent_predicted TEXT,
            confidence_score REAL,
            response_time REAL,
            accuracy_rating INTEGER,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
        
        # Ø¬Ø¯ÙˆÙ„ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS user_feedback (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_input TEXT,
            bot_response TEXT,
            user_rating INTEGER CHECK(user_rating BETWEEN 1 AND 5),
            feedback_text TEXT,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
        
        conn.commit()
        conn.close()
    
    def generate_academic_report(self):
        """Ø¥Ù†ØªØ§Ø¬ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„"""
        report = {
            "project_info": {
                "title": "Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ´Ø§ØªØ¨ÙˆØª Ø§Ù„Ø°ÙƒÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª",
                "subtitle": "Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ØªØ®Ø±Ø¬ - Ø¯Ø¹Ù… mBERT Ùˆ Transformers",
                "student_name": "Ù„ÙŠØ§Ù„",
                "technologies": ["mBERT", "Transformers", "Flask", "SQLite", "APIs"],
                "languages_supported": ["Arabic", "English", "Mixed"],
                "generated_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            },
            "technical_specifications": self.get_technical_specs(),
            "performance_metrics": self.get_performance_metrics(),
            "dataset_analysis": self.get_dataset_analysis(),
            "multilingual_support": self.get_multilingual_analysis(),
            "external_integrations": self.get_integration_analysis()
        }
        
        return report
    
    def get_technical_specs(self):
        """Ø§Ù„Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©"""
        return {
            "ai_model": "mBERT (bert-base-multilingual-cased)" if hasattr(self.chatbot, 'use_mbert') and self.chatbot.use_mbert else "Scikit-learn Pipeline",
            "database": "SQLite with 4 specialized tables",
            "api_integrations": ["wttr.in Weather API", "RSS News Feeds", "OpenWeatherMap (optional)"],
            "framework": "Flask Web Application",
            "supported_languages": ["Arabic", "English"],
            "response_types": ["Text", "Structured Data", "External APIs", "Real-time Information"]
        }
    
    def get_performance_metrics(self):
        """Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        conn = sqlite3.connect(self.chatbot.db_path)
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø«Ù‚Ø©
        df_conversations = pd.read_sql_query(
            "SELECT confidence, response_time, timestamp FROM conversations WHERE confidence > 0", 
            conn
        )
        
        if not df_conversations.empty:
            metrics = {
                "total_conversations": len(df_conversations),
                "average_confidence": df_conversations['confidence'].mean() * 100,
                "confidence_std": df_conversations['confidence'].std() * 100,
                "average_response_time": df_conversations['response_time'].mean() * 1000,  # ms
                "high_confidence_rate": len(df_conversations[df_conversations['confidence'] > 0.7]) / len(df_conversations) * 100,
                "confidence_distribution": {
                    "high (>70%)": len(df_conversations[df_conversations['confidence'] > 0.7]),
                    "medium (30-70%)": len(df_conversations[(df_conversations['confidence'] >= 0.3) & (df_conversations['confidence'] <= 0.7)]),
                    "low (<30%)": len(df_conversations[df_conversations['confidence'] < 0.3])
                }
            }
        else:
            metrics = {
                "total_conversations": 0,
                "average_confidence": 0,
                "note": "Ù„Ù… ÙŠØªÙ… Ø¥Ø¬Ø±Ø§Ø¡ Ù…Ø­Ø§Ø¯Ø«Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„"
            }
        
        conn.close()
        return metrics
    
    def get_dataset_analysis(self):
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        datasets_info = {}
        
        # ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ù„Ø§Ø¨
        try:
            students_df = pd.read_csv("students_data.csv")
            datasets_info["students"] = {
                "total_records": len(students_df),
                "unique_majors": students_df['major'].nunique(),
                "average_gpa": students_df['gpa'].mean(),
                "gpa_distribution": students_df['gpa'].describe().to_dict(),
                "university_distribution": students_df['university'].value_counts().to_dict()
            }
        except Exception as e:
            datasets_info["students"] = {"error": str(e)}
        
        # ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ù‚Ø³
        try:
            with open("weather_data.json", "r", encoding="utf-8") as f:
                weather_data = json.load(f)
                datasets_info["weather"] = {
                    "total_cities": len(weather_data),
                    "avg_temperature": sum(item['temp'] for item in weather_data) / len(weather_data),
                    "cities_covered": [item['city'] for item in weather_data],
                    "conditions_variety": len(set(item['condition'] for item in weather_data))
                }
        except Exception as e:
            datasets_info["weather"] = {"error": str(e)}
        
        return datasets_info
    
    def get_multilingual_analysis(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ø¹Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª"""
        return {
            "mbert_status": hasattr(self.chatbot, 'use_mbert') and self.chatbot.use_mbert,
            "supported_features": {
                "arabic_processing": "âœ… Ø¯Ø¹Ù… ÙƒØ§Ù…Ù„",
                "english_processing": "âœ… Ø¯Ø¹Ù… ÙƒØ§Ù…Ù„", 
                "mixed_language": "âœ… Ø¯Ø¹Ù… Ø°ÙƒÙŠ",
                "language_detection": "âœ… ØªÙ„Ù‚Ø§Ø¦ÙŠ",
                "intent_classification": "âœ… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª"
            },
            "academic_value": {
                "transformer_integration": "mBERT Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…",
                "multilingual_nlp": "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ø¨ÙŠØ¹ÙŠØ© Ù„Ù„ØºØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©",
                "cross_language_understanding": "ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¹Ø¨Ø± Ø§Ù„Ù„ØºØ§Øª",
                "practical_application": "ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù…Ù„ÙŠ Ù„Ø£Ø­Ø¯Ø« ØªÙ‚Ù†ÙŠØ§Øª NLP"
            }
        }
    
    def get_integration_analysis(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ§Ù…Ù„Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©"""
        return {
            "real_time_apis": {
                "weather_api": {"source": "wttr.in", "status": "active", "cost": "free"},
                "news_feeds": {"source": "RSS", "status": "active", "cost": "free"},
                "optional_apis": {"openweather": "configurable", "newsapi": "configurable"}
            },
            "data_sources": {
                "csv_files": ["students_data.csv"],
                "json_files": ["weather_data.json", "products_data.json", "companies_data.json", "restaurants_data.json"],
                "database": "chatbot.db (SQLite)",
                "external_apis": "3+ active integrations"
            },
            "academic_significance": {
                "real_world_data": "Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª",
                "multi_source_integration": "Ø¯Ù…Ø¬ Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø©",
                "scalable_architecture": "Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙˆØ³Ø¹ ÙˆØ§Ù„ØªØ·ÙˆÙŠØ±",
                "industry_standard": "Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØµÙ†Ø§Ø¹Ø© Ø§Ù„Ø­Ø¯ÙŠØ«Ø©"
            }
        }
    
    def export_academic_report(self, format="json"):
        """ØªØµØ¯ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ"""
        report = self.generate_academic_report()
        
        if format == "json":
            with open("academic_report.json", "w", encoding="utf-8") as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
        
        elif format == "markdown":
            markdown_content = self.generate_markdown_report(report)
            with open("academic_report.md", "w", encoding="utf-8") as f:
                f.write(markdown_content)
        
        return f"ØªÙ… ØªØµØ¯ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ Ø¨ØµÙŠØºØ© {format}"
    
    def generate_markdown_report(self, report):
        """Ø¥Ù†ØªØ§Ø¬ ØªÙ‚Ø±ÙŠØ± Markdown Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ"""
        markdown = f"""# {report['project_info']['title']}

## {report['project_info']['subtitle']}

**Ø§Ù„Ø·Ø§Ù„Ø¨Ø©:** {report['project_info']['student_name']}  
**ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù†ØªØ§Ø¬:** {report['project_info']['generated_date']}

## Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
{', '.join(report['project_info']['technologies'])}

## Ø§Ù„Ù…ÙˆØ§ØµÙØ§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©

### Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
- **Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:** {report['technical_specifications']['ai_model']}
- **Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:** {report['technical_specifications']['database']}
- **Ø§Ù„Ø¥Ø·Ø§Ø±:** {report['technical_specifications']['framework']}

### Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ù„ØºÙˆÙŠ
- **Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©:** {', '.join(report['technical_specifications']['supported_languages'])}

## Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡

- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª:** {report['performance_metrics'].get('total_conversations', 0)}
- **Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©:** {report['performance_metrics'].get('average_confidence', 0):.2f}%
- **Ù…ØªÙˆØ³Ø· Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©:** {report['performance_metrics'].get('average_response_time', 0):.2f} ms

## Ø§Ù„ØªÙƒØ§Ù…Ù„Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©

### APIs Ø§Ù„Ù…ÙØ¹Ù„Ø©
- wttr.in Ù„Ù„Ø·Ù‚Ø³ (Ù…Ø¬Ø§Ù†ÙŠ)
- RSS Ù„Ù„Ø£Ø®Ø¨Ø§Ø± (Ù…Ø¬Ø§Ù†ÙŠ)
- OpenWeatherMap (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)

## Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©

Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙŠÙ…Ø«Ù„ ØªØ·Ø¨ÙŠÙ‚Ø§Ù‹ Ù…ØªÙ‚Ø¯Ù…Ø§Ù‹ Ù„ØªÙ‚Ù†ÙŠØ§Øª:
- **Transformers Ùˆ mBERT**
- **Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù„ØºØ§Øª** 
- **ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©**
- **ØªØ·ÙˆÙŠØ± ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ÙˆÙŠØ¨ Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©**

---
*ØªÙ… Ø¥Ù†ØªØ§Ø¬ Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨ÙˆØ§Ø³Ø·Ø© Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ*
"""
        return markdown

# Ø¯Ù…Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ© ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
def integrate_academic_features(chatbot_instance):
    """Ø¯Ù…Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©"""
    academic_analytics = AcademicAnalytics(chatbot_instance)
    return academic_analytics
