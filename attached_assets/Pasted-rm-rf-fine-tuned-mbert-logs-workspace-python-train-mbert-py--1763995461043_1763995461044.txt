rm -rf fine_tuned_mbert logs
~/workspace$ python train_mbert.py
ğŸ“š Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙˆØ§Ø²Ù†Ø©...

ğŸ“Š ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:
   unknown: 90 Ø¬Ù…Ù„Ø©
   help: 63 Ø¬Ù…Ù„Ø©
   greeting: 60 Ø¬Ù…Ù„Ø©
   about: 57 Ø¬Ù…Ù„Ø©
   medicine: 53 Ø¬Ù…Ù„Ø©
   weather: 31 Ø¬Ù…Ù„Ø©
   goodbye: 25 Ø¬Ù…Ù„Ø©
   time: 24 Ø¬Ù…Ù„Ø©

ğŸ“ˆ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª: 403
ğŸ·ï¸ Ø¹Ø¯Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª: 8
ğŸ“‹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª: ['about', 'goodbye', 'greeting', 'help', 'medicine', 'time', 'unknown', 'weather']

ğŸ”„ ØªØ­Ù…ÙŠÙ„ mBERT...
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
âš™ï¸ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ...
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 403/403 [00:00<00:00, 4681.82 examples/s]
ğŸ‹ï¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: 322 Ø¹ÙŠÙ†Ø©
ğŸ§ª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: 81 Ø¹ÙŠÙ†Ø©
/home/runner/workspace/train_mbert.py:293: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(

ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­Ø³Ù†...
  0%|                                                                                                                        | 0/492 [00:00<?, ?it/s]/home/runner/workspace/.pythonlibs/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
  warnings.warn(warn_msg)
{'loss': 2.0629, 'grad_norm': 7.300604343414307, 'learning_rate': 2.666666666666667e-07, 'epoch': 0.12}                                              
{'loss': 2.0568, 'grad_norm': 6.357724666595459, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.24}                                              
{'loss': 2.0576, 'grad_norm': 6.754056930541992, 'learning_rate': 9.333333333333334e-07, 'epoch': 0.37}                                              
{'loss': 2.0697, 'grad_norm': 6.011123180389404, 'learning_rate': 1.2666666666666669e-06, 'epoch': 0.49}                                             
  4%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ